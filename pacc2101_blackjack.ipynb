{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9LIvGdraWNpjfkAQuvrb6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chrispako990210/GRO860/blob/master/pacc2101_blackjack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBg4lN0tLZAu",
        "outputId": "383e676f-8b49-4f90-af02-5dd2f4eb8d70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting stable-baselines3\n",
            "  Downloading stable_baselines3-2.4.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (1.26.4)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.5.1+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.1.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable-baselines3) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.13->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (4.55.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (11.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->stable-baselines3) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable-baselines3) (3.0.2)\n",
            "Downloading stable_baselines3-2.4.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.4.0\n"
          ]
        }
      ],
      "source": [
        "# Download packages to session\n",
        "\n",
        "!pip install gymnasium\n",
        "!pip install numpy\n",
        "!pip install stable-baselines3\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import random\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "\n",
        "# Useful functions for card manipulations and calculations\n",
        "\n",
        "def compare(a, b):\n",
        "    if a == b:\n",
        "        return -1.0 # Tie, dealer still wins\n",
        "    return float(a > b) - float(a < b)\n",
        "\n",
        "def has_usable_ace(hand):\n",
        "    return int(1 in hand and sum(hand) + 10 <= 21)\n",
        "\n",
        "def get_hand_value(hand):\n",
        "    if has_usable_ace(hand):\n",
        "        return sum(hand) + 10\n",
        "    return sum(hand)\n",
        "\n",
        "def is_bust(hand):\n",
        "    return get_hand_value(hand) > 21\n",
        "\n",
        "def score(hand):\n",
        "    return 0 if is_bust(hand) else get_hand_value(hand)\n",
        "\n",
        "# Creating are custom environment\n",
        "class CustomBlackjack(gym.Env):\n",
        "\n",
        "    def __init__(self, n_decks=3, n_shuffle=0.25):\n",
        "        super().__init__()\n",
        "        self.n_decks = n_decks\n",
        "        self.deck = self.initialize_deck()\n",
        "        self.min_cards_left = int(n_shuffle * len(self.deck))\n",
        "        self.initial_bankroll = 200.0\n",
        "        self.target = self.initial_bankroll * 1.5\n",
        "        self.min_bet = 0\n",
        "        self.max_bet = 50\n",
        "        self.penality = self.max_bet * 2.0\n",
        "        self.card_count = 0\n",
        "        self.card_norm = 50\n",
        "\n",
        "        self.bets = list(range(self.min_bet, self.max_bet + 1, 10))\n",
        "        self.moves = [0, 1, 2]  # Hit, Stand, Double Down\n",
        "\n",
        "        self.games_played = 0\n",
        "        self.episode_count = 0\n",
        "\n",
        "        self.action_space = gym.spaces.Box(\n",
        "            low=np.array([self.min_bet, 0]),\n",
        "            high=np.array([self.max_bet, 2]),\n",
        "            shape=(2,),\n",
        "            dtype=np.float32\n",
        "            )\n",
        "\n",
        "        # Observations mapped as [is_betting_round [0, 1], dealer card [1,10], agent sum [4, 21], usable_ace [0, 1], bankroll [0, 2000], card_count [-1000, 1000]]\n",
        "        # TODO: Maybe add min and max bets as static values in the observation space, this may help the agent to learn the limits of the game and avoid invalid bets conditions.\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=np.array([0, 0, 0, 0, 0, -self.card_norm, self.max_bet]),\n",
        "            high=np.array([1, 10, 21, 1, self.target, self.card_norm, self.max_bet]),\n",
        "            shape=(7,),\n",
        "            dtype=np.int16\n",
        "            )\n",
        "\n",
        "    def _map_action(self, action):\n",
        "        # Map action to nearest valid bet and move\n",
        "        bet = min(self.bets, key=lambda x: abs(x - action[0]))\n",
        "        move = int(np.round(np.clip(action[1], 0, 2)))\n",
        "        return bet, move\n",
        "\n",
        "    def reset(self, seed=None):\n",
        "        # Initialize/reset all relevant game state variables\n",
        "        super().reset(seed=seed)\n",
        "        self.deck = self.initialize_deck()\n",
        "\n",
        "        # Reset bankroll and game variables\n",
        "        self.bankroll = self.initial_bankroll\n",
        "        self.current_bet = 0\n",
        "        self.betting_round = 1\n",
        "\n",
        "        # Reset blackjack state\n",
        "        self.agent_hand = self.draw_hand()\n",
        "        self.dealer_hand = self.draw_hand(count_card=False)\n",
        "\n",
        "        # Update counters\n",
        "        self.episode_count += 1\n",
        "        # Info for debugging and\n",
        "        info = {\"episode_number\": self.episode_count,\n",
        "                \"hands_played\": self.games_played,\n",
        "                \"bankroll\": self.bankroll}\n",
        "        self.games_played = 0\n",
        "\n",
        "        return self.get_observations(is_start = True), info\n",
        "\n",
        "    def _normalize_observation(self, observation):\n",
        "        # Betting round: already binary, no change needed\n",
        "        betting_round = observation[0]\n",
        "\n",
        "        # Dealer card: normalize from [0, 10] to [0, 1]\n",
        "        dealer_card = observation[1] / 10\n",
        "\n",
        "        # Agent sum: normalize from [0, 21] to [0, 1]\n",
        "        agent_sum = observation[2] / 21\n",
        "\n",
        "        # Usable ace: already binary, no change needed\n",
        "        usable_ace = observation[3]\n",
        "\n",
        "        # Bankroll: normalize based on initial and max bankroll\n",
        "        bankroll = observation[4] / self.target\n",
        "\n",
        "        # Card count: normalize to [-1, 1] range\n",
        "        card_count = observation[5] / self.card_norm\n",
        "\n",
        "        return np.array([\n",
        "            betting_round,\n",
        "            dealer_card,\n",
        "            agent_sum,\n",
        "            usable_ace,\n",
        "            bankroll,\n",
        "            card_count,\n",
        "            (self.max_bet / (self.target))\n",
        "        ], np.float32)\n",
        "\n",
        "    def get_observations(self, is_start = False):\n",
        "        if is_start:\n",
        "            return np.array([1, 0, 0, 0, 0.5, 0, self.max_bet/self.target], dtype=np.float32)\n",
        "        else:\n",
        "            obs = np.array([\n",
        "                self.betting_round,\n",
        "                0 if self.betting_round else self.dealer_hand[0],\n",
        "                0 if self.betting_round else get_hand_value(self.agent_hand),\n",
        "                has_usable_ace(self.agent_hand),\n",
        "                self.bankroll,\n",
        "                self.card_count,\n",
        "            ])\n",
        "            # Apply normalization here\n",
        "            return self._normalize_observation(obs)\n",
        "\n",
        "    def initialize_deck(self):\n",
        "        self.card_count = 0\n",
        "        self.games_played = 0\n",
        "        self.deck = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10] * 4 * self.n_decks\n",
        "        random.shuffle(self.deck) # Shuffle the deck at the beginning\n",
        "        return deque(self.deck)\n",
        "\n",
        "    def draw_card(self, count_card=True):\n",
        "        if len(self.deck) <= self.min_cards_left:\n",
        "            self.deck = self.initialize_deck()\n",
        "        card = self.deck.popleft()\n",
        "        if count_card:\n",
        "            self.update_count(card)\n",
        "        return card\n",
        "\n",
        "    def draw_hand(self, count_card=True):\n",
        "        return [self.draw_card(), self.draw_card(count_card)]\n",
        "\n",
        "    def update_count(self, card):\n",
        "        if card in [2, 3, 4, 5, 6]:\n",
        "            self.card_count += 1\n",
        "        elif card in [10, 1]:\n",
        "            self.card_count -= 1\n",
        "\n",
        "    # Not implemented, could help exploration and learning of game mechanics\n",
        "    def card_count_reward_watchdog(self, move):\n",
        "        bet_high_i = int((2/3) * len(self.bets)) - 1\n",
        "        bet_low_i = int((1/3) * len(self.bets)) - 1\n",
        "        high_bet = self.bets[bet_high_i]\n",
        "        low_bet = self.bets[bet_low_i]\n",
        "        tc_threshold = 5\n",
        "        # Good decisions\n",
        "        if self.card_count >= tc_threshold and self.current_bet >= high_bet:\n",
        "            if move in [0, 1]:\n",
        "                return 1.0 # Reward for placing a correct bet\n",
        "            elif move == 2:\n",
        "                return 2.0 # Reward for placing a correct bet and doubling down\n",
        "        elif self.card_count <= -tc_threshold and self.current_bet <= low_bet:\n",
        "            return 1.0 # Reward for placing a correct bet\n",
        "\n",
        "        # Bad decisions\n",
        "        if self.card_count >= tc_threshold and self.current_bet <= low_bet:\n",
        "            if move in [0, 1]:\n",
        "                return -1.0 # Penality for placing a wrong bet\n",
        "            elif move == 2:\n",
        "                return -2.0\n",
        "        elif self.card_count <= -tc_threshold and self.current_bet >= high_bet:\n",
        "            if move in [0, 1]:\n",
        "                return -2.0\n",
        "            elif move == 2:\n",
        "                return -4.0\n",
        "        return 0.0\n",
        "\n",
        "    def step(self, action):\n",
        "        bet, move = self._map_action(action)\n",
        "        self.current_bet = bet\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        reward = 0.0\n",
        "        info = {}\n",
        "\n",
        "        if self.betting_round:\n",
        "            if not (bet <= min(self.max_bet, self.bankroll)):\n",
        "                truncated = True\n",
        "                reward -= self.penality\n",
        "                info = {\"outcome\": \"Invalid bet\"}\n",
        "            else:\n",
        "                self.betting_round = 0\n",
        "            return self.get_observations(), reward, terminated, truncated, info\n",
        "\n",
        "        if move == 0:  # Hit\n",
        "            self.agent_hand.append(self.draw_card())\n",
        "            if is_bust(self.agent_hand):  # Bust\n",
        "                reward -= self.current_bet\n",
        "                self.bankroll -= self.current_bet\n",
        "                self.end_game()\n",
        "\n",
        "        elif move == 1:  # Stand\n",
        "            while get_hand_value(self.dealer_hand) < 17:\n",
        "                self.dealer_hand.append(self.draw_card())\n",
        "            # update the count for the hidden card that is now revealed\n",
        "            self.update_count(self.dealer_hand[1])\n",
        "            result = compare(score(self.agent_hand), score(self.dealer_hand))\n",
        "            reward += (result * self.current_bet)\n",
        "            self.bankroll += (result * self.current_bet)\n",
        "            self.end_game()\n",
        "\n",
        "        elif move == 2:  # Double Down\n",
        "            self.current_bet = max(self.current_bet*2, self.bankroll) # Here we clip the value so the agent doesnt have to worry about it.\n",
        "            self.agent_hand.append(self.draw_card())\n",
        "            if is_bust(self.agent_hand):  # Bust\n",
        "                reward -= self.current_bet\n",
        "                self.bankroll -= self.current_bet\n",
        "            else:\n",
        "                while get_hand_value(self.dealer_hand) < 17:\n",
        "                    self.dealer_hand.append(self.draw_card())\n",
        "                # update the count for the hidden card that is now revealed\n",
        "                self.update_count(self.dealer_hand[1])\n",
        "                result = compare(score(self.agent_hand), score(self.dealer_hand))\n",
        "                reward += (result * self.current_bet)\n",
        "                self.bankroll += (result * self.current_bet)\n",
        "            self.end_game()\n",
        "        else:\n",
        "            truncated = True\n",
        "            reward -= self.penality\n",
        "            info = {\"outcome\": \"Invalid move\"}\n",
        "        # We might want to implement additional termination conditions and rewards here\n",
        "        if self.bankroll <= 0:\n",
        "            reward -= self.current_bet\n",
        "            terminated = True\n",
        "            info = {\"outcome\": \"Bankrupt\"}\n",
        "        elif self.bankroll >= self.target:\n",
        "            reward += self.current_bet\n",
        "            terminated = True\n",
        "            info = {\"outcome\": \"Profit\"}\n",
        "\n",
        "        return self.get_observations(), reward, terminated, truncated, info\n",
        "\n",
        "    def end_game(self):\n",
        "        self.games_played += 1\n",
        "        self.betting_round = 1\n"
      ],
      "metadata": {
        "id": "RV1EVX_iMAzl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training section\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv\n",
        "from stable_baselines3.common.vec_env import VecMonitor\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "from stable_baselines3.common.callbacks import EvalCallback\n",
        "from stable_baselines3.common.utils import set_random_seed\n",
        "import os\n",
        "\n",
        "n_envs = 4\n",
        "n_steps = 1024\n",
        "total_timesteps = n_steps * 150 * n_envs\n",
        "entropy = 0.005\n",
        "gamma = 0.999\n",
        "\n",
        "def make_env(rank, seed):\n",
        "    def _init():\n",
        "        env = CustomBlackjack()\n",
        "        set_random_seed(seed + rank)\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "seed = 42\n",
        "env = SubprocVecEnv([make_env(i, seed) for i in range(n_envs)])\n",
        "env = VecMonitor(env)\n",
        "model = PPO(\"MlpPolicy\",\n",
        "            env,\n",
        "            learning_rate=0.0001,\n",
        "            verbose=1,\n",
        "            ent_coef=entropy,\n",
        "            gamma=gamma, tensorboard_log=\"./logs_ppo/\",\n",
        "            device=\"cpu\",\n",
        "            n_steps=n_steps\n",
        ")\n",
        "\n",
        "model_dir = os.path.join(os.getcwd(), \"models\")\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "eval_env = DummyVecEnv([lambda: CustomBlackjack()]) # Single environment for evaluation\n",
        "eval_env = VecMonitor(eval_env)\n",
        "eval_cb = EvalCallback(\n",
        "    eval_env,\n",
        "    best_model_save_path=model_dir,\n",
        "    log_path = \"./logs_ppo/eval_logs\",\n",
        "    eval_freq=500,\n",
        "    deterministic=True,\n",
        "    render=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=total_timesteps, log_interval=10, callback=eval_cb)\n",
        "\n",
        "# Save the model\n",
        "model.save(\"ppo_mlp_blackjack\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "id": "4ax6vTDI3ZYd",
        "outputId": "bbbe29f9-99b4-4fd5-b89b-7e1a7a2215e1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Logging to ./logs_ppo/PPO_1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-84a3b2581e3c>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_cb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Save the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     ) -> SelfPPO:\n\u001b[0;32m--> 311\u001b[0;31m         return super().learn(\n\u001b[0m\u001b[1;32m    312\u001b[0m             \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0mcontinue_training\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_rollouts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrollout_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rollout_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontinue_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;31m# Give access to local variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_locals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36mon_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/callbacks.py\u001b[0m in \u001b[0;36m_on_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_success_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m             episode_rewards, episode_lengths = evaluate_policy(\n\u001b[0m\u001b[1;32m    465\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_env\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         )\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mnew_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0mcurrent_rewards\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mcurrent_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/base_vec_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \"\"\"\n\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/vec_monitor.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mVecEnvStepReturn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvenv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_returns\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_lengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\u001b[0m in \u001b[0;36mstep_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m# Avoid circular imports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0menv_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0menv_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             )\n",
            "\u001b[0;32m<ipython-input-2-9691e9149310>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmove\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Hit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_hand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_card\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mis_bust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent_hand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Bust\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_bet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbankroll\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_bet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9691e9149310>\u001b[0m in \u001b[0;36mis_bust\u001b[0;34m(hand)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_bust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mget_hand_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m21\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-9691e9149310>\u001b[0m in \u001b[0;36mget_hand_value\u001b[0;34m(hand)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_usable_ace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mis_bust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained agent and performe inference for performance evaluation based on termination conditions.\n",
        "path = os.path.join(os.getcwd(), \"models\", \"best_model.zip\")\n",
        "model = PPO.load(path, device=\"cpu\")\n",
        "\n",
        "# Initialize variables to keep track of the agent's performance\n",
        "bankrupt = 0\n",
        "doubled = 0\n",
        "bad_move = 0\n",
        "bad_bet = 0\n",
        "total_rewards = []\n",
        "env = CustomBlackjack()\n",
        "\n",
        "n_episodes = 1000\n",
        "for episode in range(n_episodes):\n",
        "    obs, _ = env.reset()\n",
        "    done = False\n",
        "    truncated = False\n",
        "    episode_reward = 0\n",
        "    while not (done or truncated):\n",
        "        # Get action from the model\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        # Step the environment\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        # Update the episode reward\n",
        "        episode_reward += reward\n",
        "\n",
        "        if info.get(\"outcome\") == \"Bankrupt\":\n",
        "            bankrupt += 1\n",
        "        if info.get(\"outcome\") == \"Invalid move\":\n",
        "            bad_move += 1\n",
        "        if info.get(\"outcome\") == \"Invalid bet\":\n",
        "            bad_bet += 1\n",
        "        if info.get(\"outcome\") == \"Profit\":\n",
        "            doubled += 1\n",
        "\n",
        "    total_rewards.append(episode_reward)\n",
        "\n",
        "# Calculate and display statistics\n",
        "print(f\"Evaluation over {n_episodes} episodes:\")\n",
        "print(f\"Bankroll lost: {bankrupt} times\")\n",
        "print(f\"Bankroll x1.5: {doubled} times\")\n",
        "print(f\"Invalid moves: {bad_move} times\")\n",
        "print(f\"Invalid bets: {bad_bet} times\")\n",
        "print(f\"Average reward per episode: {np.mean(total_rewards):.2f}\")\n",
        "print(f\"Total rewards per episode: {total_rewards}\")"
      ],
      "metadata": {
        "id": "Si0V3UW73atj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}